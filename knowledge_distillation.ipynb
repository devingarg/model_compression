{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf859501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ee52d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "train_dataset = CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False, num_workers=1)\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ddaa77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b43affbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, lr, writer, start=0, test_every=5):\n",
    "\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images.to(device))\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        writer.add_scalar(\"Training loss\", avg_loss, start+epoch+1)\n",
    "        print(f'Epoch {start + epoch + 1}/{start + num_epochs}, Loss: {avg_loss:.3f}')\n",
    "\n",
    "        if (start+epoch+1) % test_every == 0:\n",
    "            acc = evaluate(model)\n",
    "            writer.add_scalar(\"Test acc\", acc, start+epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "299be61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distil(student, teacher, teaching_wt, num_epochs, lr, distil_loss, writer, start=0, test_every=5):\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "\n",
    "    print(f\"Distillation loss: {distil_loss}; learning rate: {lr}\")\n",
    "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_label_loss = 0.0\n",
    "        running_teaching_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            teacher_output = teacher(images.to(device))\n",
    "            outputs = student(images.to(device))\n",
    "\n",
    "            label_loss = criterion(outputs, labels)\n",
    "            \n",
    "            if isinstance(distil_loss, nn.KLDivLoss):\n",
    "                # KLDivergence loss is applied on probabilities\n",
    "                teacher_output = F.softmax(teacher_output, dim=-1)\n",
    "                outputs = F.log_softmax(outputs, dim=-1)\n",
    "\n",
    "            teaching_loss = distil_loss(outputs, teacher_output)\n",
    "\n",
    "            loss = label_loss + teaching_wt * teaching_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_label_loss += label_loss.item()\n",
    "            running_teaching_loss += teaching_loss.item()\n",
    "        \n",
    "        avg_label_loss = running_label_loss / len(train_loader)\n",
    "        avg_teaching_loss = running_teaching_loss / len(train_loader)\n",
    "        avg_overall_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        writer.add_scalar(\"Label loss\", avg_label_loss, start+epoch+1)\n",
    "        writer.add_scalar(\"KnowDist loss\", avg_teaching_loss, start+epoch+1)\n",
    "        writer.add_scalar(\"Training loss\", avg_overall_loss, start+epoch+1)\n",
    "        \n",
    "        print(f'Epoch {start + epoch + 1}/{start + num_epochs}, Label: {avg_label_loss:.3f}, \\\n",
    "        Teacher: {avg_teaching_loss:.3f}, \\\n",
    "        Overall: {avg_overall_loss:.3f}')\n",
    "\n",
    "        if (start+epoch+1) % test_every == 0:\n",
    "            acc = evaluate(student)\n",
    "            writer.add_scalar(\"Test acc\", acc, start+epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c6cc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Student model is to be trained\n",
    "Experiments:\n",
    "1. 60 epochs lr 0.001\n",
    "2. 20 epochs with lr = 0.001 then 40 epochs with lr = 0.0001\n",
    "3. 20 epochs with lr = 0.001 then 40 epochs with distillation (MSELoss) lr = 0.0001\n",
    "4. 20 epochs with lr = 0.001 then 40 epochs with distillation (KLDivLoss) lr = 0.0001\n",
    "\n",
    "Log test loss every 5 epochs\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a647a",
   "metadata": {},
   "source": [
    "### Varying learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b324110",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.605\n",
      "Epoch 2/20, Loss: 1.314\n",
      "Epoch 3/20, Loss: 1.186\n",
      "Epoch 4/20, Loss: 1.110\n",
      "Epoch 5/20, Loss: 1.045\n",
      "Test Accuracy: 59.59%\n",
      "Epoch 6/20, Loss: 0.993\n",
      "Epoch 7/20, Loss: 0.953\n",
      "Epoch 8/20, Loss: 0.909\n",
      "Epoch 9/20, Loss: 0.873\n",
      "Epoch 10/20, Loss: 0.844\n",
      "Test Accuracy: 63.14%\n",
      "Epoch 11/20, Loss: 0.815\n",
      "Epoch 12/20, Loss: 0.787\n",
      "Epoch 13/20, Loss: 0.762\n",
      "Epoch 14/20, Loss: 0.740\n",
      "Epoch 15/20, Loss: 0.716\n",
      "Test Accuracy: 63.04%\n",
      "Epoch 16/20, Loss: 0.697\n",
      "Epoch 17/20, Loss: 0.673\n",
      "Epoch 18/20, Loss: 0.659\n",
      "Epoch 19/20, Loss: 0.643\n",
      "Epoch 20/20, Loss: 0.622\n",
      "Test Accuracy: 63.29%\n",
      "Epoch 21/20, Loss: 0.492\n",
      "Epoch 22/20, Loss: 0.470\n",
      "Epoch 23/20, Loss: 0.459\n",
      "Epoch 24/20, Loss: 0.452\n",
      "Epoch 25/20, Loss: 0.445\n",
      "Test Accuracy: 64.12%\n",
      "Epoch 26/20, Loss: 0.439\n",
      "Epoch 27/20, Loss: 0.434\n",
      "Epoch 28/20, Loss: 0.427\n",
      "Epoch 29/20, Loss: 0.423\n",
      "Epoch 30/20, Loss: 0.418\n",
      "Test Accuracy: 63.91%\n",
      "Epoch 31/20, Loss: 0.414\n",
      "Epoch 32/20, Loss: 0.409\n",
      "Epoch 33/20, Loss: 0.404\n",
      "Epoch 34/20, Loss: 0.400\n",
      "Epoch 35/20, Loss: 0.396\n",
      "Test Accuracy: 63.69%\n",
      "Epoch 36/20, Loss: 0.392\n",
      "Epoch 37/20, Loss: 0.388\n",
      "Epoch 38/20, Loss: 0.384\n",
      "Epoch 39/20, Loss: 0.380\n",
      "Epoch 40/20, Loss: 0.376\n",
      "Test Accuracy: 63.46%\n",
      "Epoch 41/20, Loss: 0.358\n",
      "Epoch 42/20, Loss: 0.356\n",
      "Epoch 43/20, Loss: 0.356\n",
      "Epoch 44/20, Loss: 0.355\n",
      "Epoch 45/20, Loss: 0.354\n",
      "Test Accuracy: 63.59%\n",
      "Epoch 46/20, Loss: 0.354\n",
      "Epoch 47/20, Loss: 0.354\n",
      "Epoch 48/20, Loss: 0.353\n",
      "Epoch 49/20, Loss: 0.353\n",
      "Epoch 50/20, Loss: 0.352\n",
      "Test Accuracy: 63.60%\n",
      "Epoch 51/20, Loss: 0.352\n",
      "Epoch 52/20, Loss: 0.351\n",
      "Epoch 53/20, Loss: 0.351\n",
      "Epoch 54/20, Loss: 0.350\n",
      "Epoch 55/20, Loss: 0.350\n",
      "Test Accuracy: 63.58%\n",
      "Epoch 56/20, Loss: 0.350\n",
      "Epoch 57/20, Loss: 0.349\n",
      "Epoch 58/20, Loss: 0.349\n",
      "Epoch 59/20, Loss: 0.348\n",
      "Epoch 60/20, Loss: 0.348\n",
      "Test Accuracy: 63.51%\n"
     ]
    }
   ],
   "source": [
    "w1 = SummaryWriter(\"distill_logs/lr1e-3_1e-4_1e-5\")\n",
    "\n",
    "student = LeNet(num_classes=10).to(device)\n",
    "lr = 0.001\n",
    "\n",
    "# Train for 20 epochs with lr=1e-3\n",
    "train(student, num_epochs=20, lr=lr, writer=w1)\n",
    "\n",
    "# 20 more epochs with lr=1e-4\n",
    "train(student, num_epochs=20, lr=lr/10, writer=w1, start=20)\n",
    "\n",
    "# 20 more epochs with lr=1e-5\n",
    "train(student, num_epochs=20, lr=lr/100, writer=w1, start=40)\n",
    "\n",
    "\n",
    "w1.add_text(\"Notes\", \"[1-20] LR=1e-3\\n[21-40] LR=1e-4\\n[41-60] LR=1e-5\", global_step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8cb0e1",
   "metadata": {},
   "source": [
    "### Knowledge distillation (KLDivergence loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8908c438",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.600\n",
      "Epoch 2/20, Loss: 1.308\n",
      "Epoch 3/20, Loss: 1.191\n",
      "Epoch 4/20, Loss: 1.111\n",
      "Epoch 5/20, Loss: 1.050\n",
      "Test Accuracy: 61.76%\n",
      "Epoch 6/20, Loss: 0.996\n",
      "Epoch 7/20, Loss: 0.960\n",
      "Epoch 8/20, Loss: 0.912\n",
      "Epoch 9/20, Loss: 0.876\n",
      "Epoch 10/20, Loss: 0.844\n",
      "Test Accuracy: 64.10%\n",
      "Epoch 11/20, Loss: 0.812\n",
      "Epoch 12/20, Loss: 0.782\n",
      "Epoch 13/20, Loss: 0.758\n",
      "Epoch 14/20, Loss: 0.730\n",
      "Epoch 15/20, Loss: 0.707\n",
      "Test Accuracy: 63.66%\n",
      "Epoch 16/20, Loss: 0.688\n",
      "Epoch 17/20, Loss: 0.662\n",
      "Epoch 18/20, Loss: 0.638\n",
      "Epoch 19/20, Loss: 0.624\n",
      "Epoch 20/20, Loss: 0.602\n",
      "Test Accuracy: 62.60%\n",
      "Distillation loss: KLDivLoss(); learning rate: 0.0001\n",
      "Epoch 21/60, Label: 0.472,         Teacher: 0.461,         Overall: 0.933\n",
      "Epoch 22/60, Label: 0.451,         Teacher: 0.439,         Overall: 0.890\n",
      "Epoch 23/60, Label: 0.442,         Teacher: 0.429,         Overall: 0.872\n",
      "Epoch 24/60, Label: 0.436,         Teacher: 0.423,         Overall: 0.859\n",
      "Epoch 25/60, Label: 0.429,         Teacher: 0.417,         Overall: 0.846\n",
      "Test Accuracy: 64.00%\n",
      "Epoch 26/60, Label: 0.424,         Teacher: 0.412,         Overall: 0.836\n",
      "Epoch 27/60, Label: 0.420,         Teacher: 0.407,         Overall: 0.827\n",
      "Epoch 28/60, Label: 0.414,         Teacher: 0.403,         Overall: 0.818\n",
      "Epoch 29/60, Label: 0.410,         Teacher: 0.399,         Overall: 0.809\n",
      "Epoch 30/60, Label: 0.406,         Teacher: 0.395,         Overall: 0.802\n",
      "Test Accuracy: 63.75%\n",
      "Epoch 31/60, Label: 0.402,         Teacher: 0.392,         Overall: 0.794\n",
      "Epoch 32/60, Label: 0.398,         Teacher: 0.388,         Overall: 0.786\n",
      "Epoch 33/60, Label: 0.394,         Teacher: 0.385,         Overall: 0.780\n",
      "Epoch 34/60, Label: 0.391,         Teacher: 0.382,         Overall: 0.773\n",
      "Epoch 35/60, Label: 0.387,         Teacher: 0.380,         Overall: 0.767\n",
      "Test Accuracy: 63.53%\n",
      "Epoch 36/60, Label: 0.383,         Teacher: 0.376,         Overall: 0.760\n",
      "Epoch 37/60, Label: 0.380,         Teacher: 0.374,         Overall: 0.754\n",
      "Epoch 38/60, Label: 0.377,         Teacher: 0.372,         Overall: 0.749\n",
      "Epoch 39/60, Label: 0.373,         Teacher: 0.369,         Overall: 0.742\n",
      "Epoch 40/60, Label: 0.370,         Teacher: 0.366,         Overall: 0.736\n",
      "Test Accuracy: 63.42%\n",
      "Epoch 41/60, Label: 0.367,         Teacher: 0.364,         Overall: 0.730\n",
      "Epoch 42/60, Label: 0.364,         Teacher: 0.361,         Overall: 0.725\n",
      "Epoch 43/60, Label: 0.361,         Teacher: 0.360,         Overall: 0.720\n",
      "Epoch 44/60, Label: 0.358,         Teacher: 0.356,         Overall: 0.714\n",
      "Epoch 45/60, Label: 0.355,         Teacher: 0.354,         Overall: 0.709\n",
      "Test Accuracy: 62.94%\n",
      "Epoch 46/60, Label: 0.351,         Teacher: 0.352,         Overall: 0.703\n",
      "Epoch 47/60, Label: 0.349,         Teacher: 0.350,         Overall: 0.699\n",
      "Epoch 48/60, Label: 0.346,         Teacher: 0.348,         Overall: 0.694\n",
      "Epoch 49/60, Label: 0.343,         Teacher: 0.346,         Overall: 0.689\n",
      "Epoch 50/60, Label: 0.340,         Teacher: 0.344,         Overall: 0.684\n",
      "Test Accuracy: 63.03%\n",
      "Epoch 51/60, Label: 0.338,         Teacher: 0.343,         Overall: 0.681\n",
      "Epoch 52/60, Label: 0.336,         Teacher: 0.341,         Overall: 0.676\n",
      "Epoch 53/60, Label: 0.332,         Teacher: 0.338,         Overall: 0.670\n",
      "Epoch 54/60, Label: 0.330,         Teacher: 0.336,         Overall: 0.666\n",
      "Epoch 55/60, Label: 0.327,         Teacher: 0.334,         Overall: 0.661\n",
      "Test Accuracy: 62.75%\n",
      "Epoch 56/60, Label: 0.324,         Teacher: 0.333,         Overall: 0.657\n",
      "Epoch 57/60, Label: 0.322,         Teacher: 0.331,         Overall: 0.653\n",
      "Epoch 58/60, Label: 0.319,         Teacher: 0.330,         Overall: 0.649\n",
      "Epoch 59/60, Label: 0.317,         Teacher: 0.328,         Overall: 0.645\n",
      "Epoch 60/60, Label: 0.315,         Teacher: 0.326,         Overall: 0.640\n",
      "Test Accuracy: 62.59%\n"
     ]
    }
   ],
   "source": [
    "w2 = SummaryWriter(\"distill_logs/distill_KLDiv_1.0\")\n",
    "\n",
    "student = LeNet(num_classes=10).to(device)\n",
    "lr = 0.001\n",
    "\n",
    "# Train for 20 epochs with lr=1e-3\n",
    "train(student, num_epochs=20, lr=lr, writer=w2)\n",
    "\n",
    "# 40 more epochs with KLDivergence (knowledge distillation) loss (weighted 1.0) & lr=1e-4 \n",
    "train_distil(student, teacher, teaching_wt=1.0, num_epochs=40, lr=lr/10, writer=w2,\n",
    "             distil_loss=nn.KLDivLoss(reduction=\"batchmean\"), start=20, test_every=5)\n",
    "\n",
    "w2.add_text(\"Notes\", \"[1-20] LR=1e-3\\n[21-60] LR=1e-4\\nDistillation Loss: KLDivLoss()\\nWeight=1.0\", global_step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bfa7787e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.618\n",
      "Epoch 2/20, Loss: 1.317\n",
      "Epoch 3/20, Loss: 1.200\n",
      "Epoch 4/20, Loss: 1.126\n",
      "Epoch 5/20, Loss: 1.059\n",
      "Test Accuracy: 59.02%\n",
      "Epoch 6/20, Loss: 1.009\n",
      "Epoch 7/20, Loss: 0.964\n",
      "Epoch 8/20, Loss: 0.922\n",
      "Epoch 9/20, Loss: 0.885\n",
      "Epoch 10/20, Loss: 0.850\n",
      "Test Accuracy: 63.14%\n",
      "Epoch 11/20, Loss: 0.825\n",
      "Epoch 12/20, Loss: 0.795\n",
      "Epoch 13/20, Loss: 0.770\n",
      "Epoch 14/20, Loss: 0.745\n",
      "Epoch 15/20, Loss: 0.726\n",
      "Test Accuracy: 63.78%\n",
      "Epoch 16/20, Loss: 0.703\n",
      "Epoch 17/20, Loss: 0.685\n",
      "Epoch 18/20, Loss: 0.664\n",
      "Epoch 19/20, Loss: 0.644\n",
      "Epoch 20/20, Loss: 0.624\n",
      "Test Accuracy: 63.46%\n",
      "Distillation loss: KLDivLoss(); learning rate: 0.0001\n",
      "Epoch 21/60, Label: 0.496,         Teacher: 0.498,         Overall: 0.546\n",
      "Epoch 22/60, Label: 0.474,         Teacher: 0.485,         Overall: 0.522\n",
      "Epoch 23/60, Label: 0.463,         Teacher: 0.480,         Overall: 0.511\n",
      "Epoch 24/60, Label: 0.456,         Teacher: 0.477,         Overall: 0.504\n",
      "Epoch 25/60, Label: 0.449,         Teacher: 0.473,         Overall: 0.496\n",
      "Test Accuracy: 64.63%\n",
      "Epoch 26/60, Label: 0.443,         Teacher: 0.471,         Overall: 0.490\n",
      "Epoch 27/60, Label: 0.438,         Teacher: 0.469,         Overall: 0.485\n",
      "Epoch 28/60, Label: 0.433,         Teacher: 0.467,         Overall: 0.480\n",
      "Epoch 29/60, Label: 0.428,         Teacher: 0.465,         Overall: 0.474\n",
      "Epoch 30/60, Label: 0.423,         Teacher: 0.463,         Overall: 0.469\n",
      "Test Accuracy: 64.50%\n",
      "Epoch 31/60, Label: 0.419,         Teacher: 0.462,         Overall: 0.465\n",
      "Epoch 32/60, Label: 0.415,         Teacher: 0.461,         Overall: 0.461\n",
      "Epoch 33/60, Label: 0.410,         Teacher: 0.459,         Overall: 0.456\n",
      "Epoch 34/60, Label: 0.406,         Teacher: 0.458,         Overall: 0.452\n",
      "Epoch 35/60, Label: 0.402,         Teacher: 0.456,         Overall: 0.448\n",
      "Test Accuracy: 64.38%\n",
      "Epoch 36/60, Label: 0.399,         Teacher: 0.456,         Overall: 0.444\n",
      "Epoch 37/60, Label: 0.394,         Teacher: 0.454,         Overall: 0.439\n",
      "Epoch 38/60, Label: 0.390,         Teacher: 0.454,         Overall: 0.436\n",
      "Epoch 39/60, Label: 0.387,         Teacher: 0.452,         Overall: 0.432\n",
      "Epoch 40/60, Label: 0.384,         Teacher: 0.452,         Overall: 0.429\n",
      "Test Accuracy: 63.83%\n",
      "Epoch 41/60, Label: 0.380,         Teacher: 0.450,         Overall: 0.425\n",
      "Epoch 42/60, Label: 0.376,         Teacher: 0.451,         Overall: 0.421\n",
      "Epoch 43/60, Label: 0.372,         Teacher: 0.449,         Overall: 0.417\n",
      "Epoch 44/60, Label: 0.370,         Teacher: 0.449,         Overall: 0.415\n",
      "Epoch 45/60, Label: 0.366,         Teacher: 0.447,         Overall: 0.410\n",
      "Test Accuracy: 63.66%\n",
      "Epoch 46/60, Label: 0.362,         Teacher: 0.447,         Overall: 0.407\n",
      "Epoch 47/60, Label: 0.360,         Teacher: 0.447,         Overall: 0.405\n",
      "Epoch 48/60, Label: 0.357,         Teacher: 0.446,         Overall: 0.401\n",
      "Epoch 49/60, Label: 0.353,         Teacher: 0.445,         Overall: 0.397\n",
      "Epoch 50/60, Label: 0.351,         Teacher: 0.444,         Overall: 0.395\n",
      "Test Accuracy: 63.54%\n",
      "Epoch 51/60, Label: 0.347,         Teacher: 0.445,         Overall: 0.392\n",
      "Epoch 52/60, Label: 0.343,         Teacher: 0.443,         Overall: 0.387\n",
      "Epoch 53/60, Label: 0.341,         Teacher: 0.443,         Overall: 0.385\n",
      "Epoch 54/60, Label: 0.338,         Teacher: 0.443,         Overall: 0.382\n",
      "Epoch 55/60, Label: 0.335,         Teacher: 0.443,         Overall: 0.379\n",
      "Test Accuracy: 63.52%\n",
      "Epoch 56/60, Label: 0.332,         Teacher: 0.441,         Overall: 0.376\n",
      "Epoch 57/60, Label: 0.329,         Teacher: 0.442,         Overall: 0.373\n",
      "Epoch 58/60, Label: 0.326,         Teacher: 0.442,         Overall: 0.370\n",
      "Epoch 59/60, Label: 0.323,         Teacher: 0.440,         Overall: 0.367\n",
      "Epoch 60/60, Label: 0.320,         Teacher: 0.441,         Overall: 0.364\n",
      "Test Accuracy: 63.33%\n"
     ]
    }
   ],
   "source": [
    "w3 = SummaryWriter(\"distill_logs/distill_KLDiv_0.1\")\n",
    "\n",
    "student = LeNet(num_classes=10).to(device)\n",
    "lr = 1e-3\n",
    "\n",
    "# Train for 20 epochs with lr=1e-3\n",
    "train(student, num_epochs=20, lr=lr, writer=w3)\n",
    "\n",
    "# 40 more epochs with KLDivergence (knowledge distillation) loss (weighted 0.1) & lr=1e-4 \n",
    "train_distil(student, teacher, teaching_wt=0.1, num_epochs=40, lr=lr/10, writer=w3,\n",
    "             distil_loss=nn.KLDivLoss(reduction=\"batchmean\"), start=20, test_every=5)\n",
    "\n",
    "w3.add_text(\"Notes\", \"[1-20] LR=1e-3\\n[21-60] LR=1e-4\\nDistillation Loss: KLDivLoss()\\nWeight=0.1\", global_step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c54b9",
   "metadata": {},
   "source": [
    "### Knowledge distillation (MSE loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b5c8981",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.610\n",
      "Epoch 2/20, Loss: 1.324\n",
      "Epoch 3/20, Loss: 1.202\n",
      "Epoch 4/20, Loss: 1.113\n",
      "Epoch 5/20, Loss: 1.042\n",
      "Test Accuracy: 61.50%\n",
      "Epoch 6/20, Loss: 0.991\n",
      "Epoch 7/20, Loss: 0.943\n",
      "Epoch 8/20, Loss: 0.906\n",
      "Epoch 9/20, Loss: 0.866\n",
      "Epoch 10/20, Loss: 0.840\n",
      "Test Accuracy: 63.64%\n",
      "Epoch 11/20, Loss: 0.805\n",
      "Epoch 12/20, Loss: 0.777\n",
      "Epoch 13/20, Loss: 0.753\n",
      "Epoch 14/20, Loss: 0.734\n",
      "Epoch 15/20, Loss: 0.709\n",
      "Test Accuracy: 64.48%\n",
      "Epoch 16/20, Loss: 0.687\n",
      "Epoch 17/20, Loss: 0.663\n",
      "Epoch 18/20, Loss: 0.649\n",
      "Epoch 19/20, Loss: 0.627\n",
      "Epoch 20/20, Loss: 0.614\n",
      "Test Accuracy: 63.25%\n",
      "Distillation loss: MSELoss(); learning rate: 0.0001\n",
      "Epoch 21/60, Label: 0.577,         Teacher: 10.497,         Overall: 11.074\n",
      "Epoch 22/60, Label: 0.606,         Teacher: 8.983,         Overall: 9.589\n",
      "Epoch 23/60, Label: 0.625,         Teacher: 8.393,         Overall: 9.018\n",
      "Epoch 24/60, Label: 0.639,         Teacher: 8.034,         Overall: 8.672\n",
      "Epoch 25/60, Label: 0.649,         Teacher: 7.763,         Overall: 8.412\n",
      "Test Accuracy: 65.37%\n",
      "Epoch 26/60, Label: 0.658,         Teacher: 7.545,         Overall: 8.203\n",
      "Epoch 27/60, Label: 0.664,         Teacher: 7.377,         Overall: 8.041\n",
      "Epoch 28/60, Label: 0.671,         Teacher: 7.220,         Overall: 7.891\n",
      "Epoch 29/60, Label: 0.674,         Teacher: 7.093,         Overall: 7.767\n",
      "Epoch 30/60, Label: 0.678,         Teacher: 6.970,         Overall: 7.649\n",
      "Test Accuracy: 65.68%\n",
      "Epoch 31/60, Label: 0.680,         Teacher: 6.860,         Overall: 7.540\n",
      "Epoch 32/60, Label: 0.683,         Teacher: 6.768,         Overall: 7.451\n",
      "Epoch 33/60, Label: 0.686,         Teacher: 6.677,         Overall: 7.363\n",
      "Epoch 34/60, Label: 0.686,         Teacher: 6.604,         Overall: 7.290\n",
      "Epoch 35/60, Label: 0.689,         Teacher: 6.521,         Overall: 7.210\n",
      "Test Accuracy: 65.84%\n",
      "Epoch 36/60, Label: 0.689,         Teacher: 6.457,         Overall: 7.146\n",
      "Epoch 37/60, Label: 0.689,         Teacher: 6.381,         Overall: 7.070\n",
      "Epoch 38/60, Label: 0.691,         Teacher: 6.323,         Overall: 7.014\n",
      "Epoch 39/60, Label: 0.692,         Teacher: 6.264,         Overall: 6.956\n",
      "Epoch 40/60, Label: 0.693,         Teacher: 6.209,         Overall: 6.902\n",
      "Test Accuracy: 66.19%\n",
      "Epoch 41/60, Label: 0.692,         Teacher: 6.155,         Overall: 6.847\n",
      "Epoch 42/60, Label: 0.692,         Teacher: 6.106,         Overall: 6.798\n",
      "Epoch 43/60, Label: 0.694,         Teacher: 6.053,         Overall: 6.747\n",
      "Epoch 44/60, Label: 0.693,         Teacher: 6.009,         Overall: 6.702\n",
      "Epoch 45/60, Label: 0.693,         Teacher: 5.967,         Overall: 6.660\n",
      "Test Accuracy: 66.29%\n",
      "Epoch 46/60, Label: 0.692,         Teacher: 5.921,         Overall: 6.612\n",
      "Epoch 47/60, Label: 0.693,         Teacher: 5.881,         Overall: 6.575\n",
      "Epoch 48/60, Label: 0.693,         Teacher: 5.843,         Overall: 6.536\n",
      "Epoch 49/60, Label: 0.694,         Teacher: 5.802,         Overall: 6.495\n",
      "Epoch 50/60, Label: 0.692,         Teacher: 5.767,         Overall: 6.459\n",
      "Test Accuracy: 66.16%\n",
      "Epoch 51/60, Label: 0.690,         Teacher: 5.728,         Overall: 6.418\n",
      "Epoch 52/60, Label: 0.691,         Teacher: 5.699,         Overall: 6.391\n",
      "Epoch 53/60, Label: 0.689,         Teacher: 5.668,         Overall: 6.357\n",
      "Epoch 54/60, Label: 0.690,         Teacher: 5.635,         Overall: 6.325\n",
      "Epoch 55/60, Label: 0.691,         Teacher: 5.603,         Overall: 6.293\n",
      "Test Accuracy: 66.43%\n",
      "Epoch 56/60, Label: 0.690,         Teacher: 5.568,         Overall: 6.258\n",
      "Epoch 57/60, Label: 0.688,         Teacher: 5.535,         Overall: 6.223\n",
      "Epoch 58/60, Label: 0.687,         Teacher: 5.518,         Overall: 6.205\n",
      "Epoch 59/60, Label: 0.687,         Teacher: 5.487,         Overall: 6.173\n",
      "Epoch 60/60, Label: 0.686,         Teacher: 5.453,         Overall: 6.139\n",
      "Test Accuracy: 66.36%\n"
     ]
    }
   ],
   "source": [
    "w4 = SummaryWriter(\"distill_logs/distill_MSE_1.0\")\n",
    "\n",
    "student = LeNet(num_classes=10).to(device)\n",
    "lr = 1e-3\n",
    "\n",
    "# Train for 20 epochs with lr=1e-3\n",
    "train(student, num_epochs=20, lr=lr, writer=w4)\n",
    "\n",
    "# 40 more epochs with MSE (knowledge distillation) loss (weighted 1.0) & lr=1e-4 \n",
    "train_distil(student, teacher, teaching_wt=1, num_epochs=40, lr=lr/10, writer=w4,\n",
    "             distil_loss=nn.MSELoss(), start=20, test_every=5)\n",
    "\n",
    "w4.add_text(\"Notes\", \"[1-20] LR=1e-3\\n[21-60] LR=1e-4\\nDistillation Loss: MSELoss()\\nWeight=1.0\", global_step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c69fec99",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.613\n",
      "Epoch 2/20, Loss: 1.308\n",
      "Epoch 3/20, Loss: 1.184\n",
      "Epoch 4/20, Loss: 1.095\n",
      "Epoch 5/20, Loss: 1.033\n",
      "Test Accuracy: 61.94%\n",
      "Epoch 6/20, Loss: 0.975\n",
      "Epoch 7/20, Loss: 0.933\n",
      "Epoch 8/20, Loss: 0.892\n",
      "Epoch 9/20, Loss: 0.858\n",
      "Epoch 10/20, Loss: 0.823\n",
      "Test Accuracy: 63.19%\n",
      "Epoch 11/20, Loss: 0.794\n",
      "Epoch 12/20, Loss: 0.769\n",
      "Epoch 13/20, Loss: 0.744\n",
      "Epoch 14/20, Loss: 0.720\n",
      "Epoch 15/20, Loss: 0.699\n",
      "Test Accuracy: 63.27%\n",
      "Epoch 16/20, Loss: 0.678\n",
      "Epoch 17/20, Loss: 0.652\n",
      "Epoch 18/20, Loss: 0.636\n",
      "Epoch 19/20, Loss: 0.618\n",
      "Epoch 20/20, Loss: 0.604\n",
      "Test Accuracy: 62.06%\n",
      "Distillation loss: MSELoss(); learning rate: 0.0001\n",
      "Epoch 21/60, Label: 0.500,         Teacher: 11.432,         Overall: 1.643\n",
      "Epoch 22/60, Label: 0.489,         Teacher: 10.005,         Overall: 1.489\n",
      "Epoch 23/60, Label: 0.487,         Teacher: 9.482,         Overall: 1.435\n",
      "Epoch 24/60, Label: 0.485,         Teacher: 9.154,         Overall: 1.400\n",
      "Epoch 25/60, Label: 0.484,         Teacher: 8.923,         Overall: 1.376\n",
      "Test Accuracy: 64.87%\n",
      "Epoch 26/60, Label: 0.482,         Teacher: 8.726,         Overall: 1.355\n",
      "Epoch 27/60, Label: 0.480,         Teacher: 8.576,         Overall: 1.338\n",
      "Epoch 28/60, Label: 0.478,         Teacher: 8.453,         Overall: 1.324\n",
      "Epoch 29/60, Label: 0.477,         Teacher: 8.334,         Overall: 1.311\n",
      "Epoch 30/60, Label: 0.475,         Teacher: 8.224,         Overall: 1.298\n",
      "Test Accuracy: 65.13%\n",
      "Epoch 31/60, Label: 0.473,         Teacher: 8.142,         Overall: 1.288\n",
      "Epoch 32/60, Label: 0.471,         Teacher: 8.061,         Overall: 1.277\n",
      "Epoch 33/60, Label: 0.470,         Teacher: 7.985,         Overall: 1.268\n",
      "Epoch 34/60, Label: 0.468,         Teacher: 7.913,         Overall: 1.259\n",
      "Epoch 35/60, Label: 0.466,         Teacher: 7.845,         Overall: 1.250\n",
      "Test Accuracy: 65.48%\n",
      "Epoch 36/60, Label: 0.465,         Teacher: 7.780,         Overall: 1.243\n",
      "Epoch 37/60, Label: 0.463,         Teacher: 7.720,         Overall: 1.235\n",
      "Epoch 38/60, Label: 0.461,         Teacher: 7.672,         Overall: 1.228\n",
      "Epoch 39/60, Label: 0.459,         Teacher: 7.627,         Overall: 1.222\n",
      "Epoch 40/60, Label: 0.457,         Teacher: 7.569,         Overall: 1.214\n",
      "Test Accuracy: 65.27%\n",
      "Epoch 41/60, Label: 0.455,         Teacher: 7.527,         Overall: 1.208\n",
      "Epoch 42/60, Label: 0.453,         Teacher: 7.482,         Overall: 1.201\n",
      "Epoch 43/60, Label: 0.451,         Teacher: 7.442,         Overall: 1.195\n",
      "Epoch 44/60, Label: 0.450,         Teacher: 7.401,         Overall: 1.190\n",
      "Epoch 45/60, Label: 0.448,         Teacher: 7.363,         Overall: 1.184\n",
      "Test Accuracy: 65.43%\n",
      "Epoch 46/60, Label: 0.446,         Teacher: 7.324,         Overall: 1.178\n",
      "Epoch 47/60, Label: 0.444,         Teacher: 7.297,         Overall: 1.173\n",
      "Epoch 48/60, Label: 0.442,         Teacher: 7.253,         Overall: 1.167\n",
      "Epoch 49/60, Label: 0.440,         Teacher: 7.220,         Overall: 1.162\n",
      "Epoch 50/60, Label: 0.439,         Teacher: 7.201,         Overall: 1.159\n",
      "Test Accuracy: 65.25%\n",
      "Epoch 51/60, Label: 0.436,         Teacher: 7.160,         Overall: 1.152\n",
      "Epoch 52/60, Label: 0.435,         Teacher: 7.131,         Overall: 1.148\n",
      "Epoch 53/60, Label: 0.433,         Teacher: 7.110,         Overall: 1.144\n",
      "Epoch 54/60, Label: 0.431,         Teacher: 7.067,         Overall: 1.138\n",
      "Epoch 55/60, Label: 0.429,         Teacher: 7.050,         Overall: 1.134\n",
      "Test Accuracy: 65.01%\n",
      "Epoch 56/60, Label: 0.428,         Teacher: 7.017,         Overall: 1.130\n",
      "Epoch 57/60, Label: 0.426,         Teacher: 6.994,         Overall: 1.125\n",
      "Epoch 58/60, Label: 0.424,         Teacher: 6.964,         Overall: 1.120\n",
      "Epoch 59/60, Label: 0.423,         Teacher: 6.937,         Overall: 1.117\n",
      "Epoch 60/60, Label: 0.421,         Teacher: 6.914,         Overall: 1.113\n",
      "Test Accuracy: 65.14%\n"
     ]
    }
   ],
   "source": [
    "w4 = SummaryWriter(\"distill_logs/distill_MSE_0.1\")\n",
    "\n",
    "student = LeNet(num_classes=10).to(device)\n",
    "lr = 1e-3\n",
    "\n",
    "# Train for 20 epochs with lr=1e-3\n",
    "train(student, num_epochs=20, lr=lr, writer=w4)\n",
    "\n",
    "# 40 more epochs with MSE (knowledge distillation) loss (weighted 0.1) & lr=1e-4 \n",
    "train_distil(student, teacher, teaching_wt=0.1, num_epochs=40, lr=lr/10, writer=w4,\n",
    "             distil_loss=nn.MSELoss(), start=20, test_every=5)\n",
    "\n",
    "w4.add_text(\"Notes\", \"[1-20] LR=1e-3\\n[21-60] LR=1e-4\\nDistillation Loss: MSELoss()\\nWeight=0.1\", global_step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8edac3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
