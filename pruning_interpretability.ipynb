{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shap\n",
    "from PIL import Image\n",
    "\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "from copy import deepcopy \n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a0f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        sh = ((input_shape - 4)//2 - 4)//2\n",
    "        self.fc1 = nn.Linear(16 * sh * sh, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bff0779",
   "metadata": {},
   "source": [
    "## Baseline training\n",
    "\n",
    "FashionMNIST has got 10 classes.\n",
    "The training set has got 60k samples & the test set has 10k samples.\n",
    "\n",
    "Oxford IIIT Pet dataset has 37 classes.\n",
    "Training: 3680. Test: 3669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4138734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Resize((32, 32)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    \n",
    "])\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "train_dataset = datasets.OxfordIIITPet(root=\"./data\", split='trainval', transform=transform, download=True)\n",
    "test_dataset = datasets.OxfordIIITPet(root=\"./data\", split='test', transform=transform, download=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "classes = [\"Abyssinian\", \"Bengal\", \"Birman\", \"Bombay\", \"British Shorthair\", \"Egyptian Mau\", \"Maine Coon\", \"Persian\", \"Ragdoll\", \"Russian Blue\", \"Siamese\", \"Sphynx\", \"American Bulldog\", \"American Pit Bull Terrier\", \"Basset Hound\", \"Beagle\", \"Boxer\", \"Chihuahua\", \"English Cocker Spaniel\", \"English Setter\", \"German Shorthaired Pointer\", \"Great Pyrenees\", \"Havanese\", \"Japanese Chin\", \"Keeshond\", \"Leonberger\", \"Miniature Pinscher\", \"Newfoundland\", \"Pomeranian\", \"Pug\", \"Saint Bernard\", \"Samoyed\", \"Scottish Terrier\", \"Shiba Inu\", \"Staffordshire Bull Terrier\", \"Wheaten Terrier\", \"Yorkshire Terrier\"]\n",
    "classes = sorted(classes)\n",
    "print(classes)\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b786984",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of test set: {len(test_dataset)}\")\n",
    "print(f\"Size of train set: {len(train_dataset)}\")\n",
    "\n",
    "plt.imshow(train_dataset[100][0].transpose(0, 2).transpose(0, 1))\n",
    "print(classes[train_dataset[100][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c732c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, lr):\n",
    "    \n",
    "    # training loop\n",
    "    print(f\"Number of batches per epoch: {len(train_loader)}.\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images.to(device))\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader):.3f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    # Evaluate the model on the test set    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Train Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a126c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(model, name):\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"{name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet(input_shape=224, num_classes=37).to(device=device)\n",
    "\n",
    "# model = resnet18(num_classes=37).to(device)\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "model = train(model, 6, lr=lr)\n",
    "\n",
    "evaluate(model)\n",
    "\n",
    "checkpoint(model, \"original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8a413",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33331ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class evaluator_factory:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def get_evaluator(self,):\n",
    "        \n",
    "        def evaluator(x):\n",
    "            \n",
    "            # if there's no batch dim, add one\n",
    "            if len(x.shape)==3:\n",
    "                x = np.expand_dims(x, 0)\n",
    "            \n",
    "            xdim, ydim, ch = x.shape[1:]\n",
    "            \n",
    "            # the output of transform functions is channel-first\n",
    "            y = torch.zeros((x.shape[0], ch, xdim, ydim))\n",
    "            for i in range(x.shape[0]):\n",
    "                y[i] = transforms.Resize((224, 224))(transforms.ToTensor()(x[i]))\n",
    "\n",
    "            # return the model's output\n",
    "            return self.model(y.to(device)).cpu().detach().numpy()\n",
    "        \n",
    "        return evaluator\n",
    "\n",
    "\n",
    "item_idx = 900\n",
    "x, label = train_dataset[item_idx]\n",
    "x = (x.numpy().transpose(1, 2, 0))\n",
    "\n",
    "factory_obj = evaluator_factory(model)\n",
    "\n",
    "print(classes[label])\n",
    "print(classes[np.argmax(factory_obj.get_evaluator()(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029f102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_shap(x, model, max_evals):\n",
    "\n",
    "    factory_obj = evaluator_factory(model)\n",
    "    masker = shap.maskers.Image(\"blur(128,128)\", x.shape)\n",
    "    explainer = shap.Explainer(factory_obj.get_evaluator(), masker, output_names=classes)\n",
    "\n",
    "    model.eval()\n",
    "    shap_values = explainer(\n",
    "                    np.array([x]), max_evals=max_evals, batch_size=50, outputs=shap.Explanation.argsort.flip[:5]\n",
    "                )\n",
    "\n",
    "    shap.image_plot(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52f721",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a80a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_lime(x, model):\n",
    "    \n",
    "    factory_obj = evaluator_factory(model)\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    \n",
    "    model.eval()\n",
    "    explanation = explainer.explain_instance(np.array(x).astype(np.float64),\n",
    "                                             factory_obj.get_evaluator(), \n",
    "                                             top_labels=5,\n",
    "                                             hide_color=0,\n",
    "                                             num_samples=1000)\n",
    "\n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "    #                                             negative_only=True,\n",
    "                                                positive_only=False,\n",
    "                                                num_features=15,\n",
    "                                                hide_rest=False)\n",
    "    img_boundry1 = mark_boundaries(temp, mask)\n",
    "    plt.imshow(img_boundry1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e06200",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_shap(x, model, 800)\n",
    "explain_lime(x, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57771c64",
   "metadata": {},
   "source": [
    "## Prune Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_once(model, amt = 0.2):\n",
    "    \n",
    "    # which layer to prune? let's start with conv1\n",
    "    convs = [model.conv1, model.conv2]\n",
    "    fcs = [model.fc1, model.fc2, model.fc3]\n",
    "\n",
    "    # amt: how much to prune?\n",
    "    \n",
    "    # prune it \n",
    "    # n: the norm to use in computing weight importances\n",
    "    for module in convs:\n",
    "        # dim: the dimension along which to prune. 0th axis is the channel axis of the output of conv1\n",
    "        prune.ln_structured(module, name=\"weight\", amount=amt, n=2, dim=0)\n",
    "        prune.remove(module, 'weight')\n",
    "    \n",
    "    for module in fcs[:-1]:\n",
    "        # For a fully connected layer, dim=1 means input neurons\n",
    "        # dim=0 means output neurons\n",
    "        \n",
    "#         array = module.weight.detach().numpy()\n",
    "#         with np.printoptions(precision=5, suppress=True):\n",
    "#             print(array.shape)\n",
    "#             print(array)\n",
    "\n",
    "        prune.ln_structured(module, name=\"weight\", amount=amt, n=2, dim=1)\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "#         array = module.weight.detach().numpy()\n",
    "#         with np.printoptions(precision=5, suppress=True):\n",
    "#             print(array.shape)\n",
    "#             print(array)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e524defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune the model for 5 more epochs with 1/10th of the learning rate\n",
    "# note that the pruning step above added forward_pre_hooks for the pruned\n",
    "# weight tensors. So, at every forward pass, those weights that \n",
    "# have been pruned will be zeroed out while computing the model output &\n",
    "# then backpropagating.\n",
    "\n",
    "def tune(model, num_epochs):\n",
    "    return train(model, num_epochs, lr=lr/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5290c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pruned = deepcopy(model)\n",
    "model_pruned.load_state_dict(model.state_dict())\n",
    "\n",
    "model_pruned = prune_once(model_pruned, amt=0.35)\n",
    "model_pruned = prune_once(model_pruned, amt=0.35)\n",
    "\n",
    "model_tuned = deepcopy(model)\n",
    "model_tuned.load_state_dict(model_pruned.state_dict())\n",
    "tune(model_tuned, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffb02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model)\n",
    "evaluate(model_pruned)\n",
    "evaluate(model_tuned)\n",
    "\n",
    "explain_shap(x, model, 800)\n",
    "explain_shap(x, model_pruned, 800)\n",
    "explain_shap(x, model_tuned, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_lime(x, model)\n",
    "explain_lime(x, model_pruned)\n",
    "explain_lime(x, model_tuned, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289cfb7d",
   "metadata": {},
   "source": [
    "## Make the pruning stick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957fa52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the \"pruned\" model. This size should be more than the original\n",
    "# # model because the weights haven't actually been removed. They are just \n",
    "# # being masked in the forward pass. So, in addition to the weights, masks\n",
    "# # are also being saved in the state_dict.\n",
    "\n",
    "# checkpoint(model, \"pruned\")\n",
    "\n",
    "# print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the pruning stick\n",
    "print(list(module.named_parameters()))\n",
    "\n",
    "# this \"applies\" the mask to the weights and actually changes the weight tensor\n",
    "prune.remove(module, 'weight')\n",
    "\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4688407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the size of the checkpoint will be the same as the \n",
    "# original model since we don't need to save the masks anymore\n",
    "\n",
    "checkpoint(model, \"pruned_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f04ab0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4af7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
